**DEEP LEARNING BUILDING BLOCKS**
* [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), 2014, [49687 citations]

* [Additive Attention: Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473), ICLR 2015, [13149 citations]

* [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/abs/1312.6120) ICLR2014 [894 citations]

* [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909), ACL 2016, 3000 citations **BPE Paper**

* [Layer Normalization](https://arxiv.org/abs/1607.06450), Hinton, 2016, [1959 citations]

* [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022), 2016, [900 citations]

* [Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/abs/1607.01981), 2016, [22 citations]

* [All you need is a good init](https://arxiv.org/abs/1511.06422), ICLR 2016 [386 citations]

* [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412), 2017, [881 citations]

* [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515), NIPS 2017, [991 Citations]

* [L2 Regularization versus Batch and Weight Normalization](https://arxiv.org/abs/1706.05350), 2017, [80 citations]

* [Group Normalization](https://arxiv.org/abs/1803.08494), Kaiming He, 2018 [674 citations]

* [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612), 2018, [210 citations]

* [Norm matters: efficient and accurate normalization schemes in deep networks](https://arxiv.org/abs/1803.01814), NIPS 2018, [65 citations]

* [Three Mechanisms of Weight Decay Regularization](https://arxiv.org/abs/1810.12281), 2018, [45 citations]

* [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321), [ICLR 2019] [87 Citations]


**PRE-TRAINING**

* [Unifying Question Answering, Text Classification, and Regression via Span Extraction](https://arxiv.org/pdf/1904.09286.pdf)

* [BAN: Born Again Neural Networks](https://arxiv.org/abs/1805.04770) : one of the initial distillation paper

* [Multilingual Neural Machine Translation with Knowledge Distillation](https://arxiv.org/abs/1902.10461)

* [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848), 2019

* [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291), 2019

* [Semi-Supervised Sequence Modeling with Cross-View Training](https://arxiv.org/abs/1809.08370), 2018, EMNLP 2018

* [Semi-supervised Multitask Learning for Sequence Labeling](https://arxiv.org/abs/1704.07156)

* [Unsupervised Pretraining for Sequence to Sequence Learning](https://www.aclweb.org/anthology/D17-1039/), EMNLP 2017

* [ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding](https://arxiv.org/pdf/1907.12412.pdf), AAAI 2020

* [DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling](https://openreview.net/forum?id=rJeXS04FPH), ICLR 2020

* [Depth-Adaptive Transformer](https://arxiv.org/abs/1910.10073), ICLR 2020

* [FreeLB: Enhanced Adversarial Training for Natural Language Understanding](https://arxiv.org/abs/1909.11764), ICLR 2020

* [A Mutual Information Maximization Perspective of Language Representation Learning](https://arxiv.org/abs/1910.08350), ICLR 2020

* [Mogrifier LSTM](https://arxiv.org/abs/1909.01792), ICLR 2020


**FINETUNING / TRANSFER-LEARNING**

* [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461), 2020

* [Learning and Evaluating General Linguistic Intelligence](https://arxiv.org/pdf/1901.11373.pdf), 2019

* [Learning from Dialogue after Deployment: Feed Yourself, Chatbot!](https://arxiv.org/abs/1901.05415), ACL 2019

* [Syntactic Scaffolds for Semantic Structures](https://arxiv.org/abs/1808.10485), EMNLP 2018

* [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830), ACL 2019

* [Linguistic Knowledge and Transferability of Contextual Representations](https://arxiv.org/abs/1903.08855), NAACL 2019

* [An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models](https://arxiv.org/abs/1902.10547), NAACL 2019

* [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/abs/1811.01088v2), 2019

* [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://arxiv.org/abs/1903.05987), 2019

* [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751), 2019

**MISC**

* [Semi-supervised sequence tagging with bidirectional language models](https://arxiv.org/abs/1705.00108), ACL 2017

* [A PROBABILISTIC FORMULATION OF UNSUPERVISED TEXT STYLE TRANSFER](https://arxiv.org/pdf/2002.03912.pdf), ICLR 2020

* [THIEVES ON SESAME STREET! MODEL EXTRACTION OF BERT-BASED APIS](https://arxiv.org/pdf/1910.12366.pdf), ICLR 2020


**MULTI-TASK-LEARNNG**

* [MT-DNNKD: Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding](https://arxiv.org/pdf/1904.09482.pdf) **Extension of MTDNN, worth read**

* [BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671), ICML 2019

* [Latent Multi-task Architecture Learning](https://arxiv.org/abs/1705.08142), AAAI 2019

**Representation**

* [Zero-shot Word Sense Disambiguation using Sense Definition Embeddings](https://malllabiisc.github.io/publications/papers/EWISE_ACL19.pdf), ACL 2019

* [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf), 2018 [485 citations]

* [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf), 2019, [155 citations]

* [pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference](https://arxiv.org/abs/1810.08854), 2019, NAACL

* [ENCODING WORD ORDER IN COMPLEX EMBEDDINGS](https://arxiv.org/pdf/1912.12333.pdf), ICLR 2020


**NLP Downstream tasks** 

* [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635), ICLR 2019

* [Ultra-Fine Entity Typing](https://arxiv.org/abs/1807.04905), ACL 2018 [48 citations]

* [Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing](https://arxiv.org/abs/1903.02591), NAACL 2019

* [Dynamic Meta-Embeddings for Improved Sentence Representations](https://arxiv.org/abs/1804.07983), EMNLP 2018

* [Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension](https://arxiv.org/abs/1909.00109), ACL 2019

* [THE CURIOUS CASE OF NEURAL TEXT DeGENERATION](https://arxiv.org/pdf/1904.09751.pdf), ICLR 2020

**INTERPRETABILITY**

* [Is Attention Interpretable?](https://arxiv.org/abs/1906.03731), ACL 2019, [62 citations]

* [Specializing Word Embeddings (for Parsing) by Information Bottleneck](https://www.aclweb.org/anthology/D19-1276.pdf), **Best paper EMNLP 2019**

* [What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document) [Aug-2019] [87 Citations]

* [Analyzing the Structure of Attention in a Transformer Language Model](https://arxiv.org/abs/1906.04284) [ACL-2019] [25 Citations]

* [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/pdf/1905.09418.pdf) [ACL-2019] [96 Citations]

* [Towards Transparent and Explainable Attention Models](https://arxiv.org/abs/2004.14243), Mitesh Khapra, ACL 2020

* [Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words](https://arxiv.org/abs/2005.01810), ACL 2020

* [Roles and Utilization of Attention Heads in Transformer-based Neural Language Models](https://www.aclweb.org/anthology/2020.acl-main.311/), ACL 2020

* [Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT](https://arxiv.org/abs/2004.14786), ACL 2020

* [WHAT CAN NEURAL NETWORKS REASON ABOUT?](https://arxiv.org/pdf/1905.13211.pdf), ICLR 2020


**KG + NLP**

* [COMET: Commonsense Transformers for Automatic Knowledge Graph Construction](https://www.aclweb.org/anthology/P19-1470/), ACL 2019

* [Barackâ€™s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling](https://www.aclweb.org/anthology/P19-1598/), ACL 2019

* [Knowledge-Augmented Language Model and Its Application to Unsupervised Named-Entity Recognition](https://www.aclweb.org/anthology/N19-1117/), ACL 2019

* [SEQUENTIAL LATENT KNOWLEDGE SELECTION FOR KNOWLEDGE-GROUNDED DIALOGUE](https://arxiv.org/abs/2002.07510), ICLR 2020


* Partha Talukdar's work


